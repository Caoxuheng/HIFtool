import torch
import torch.nn as nn
from .MetaTrain_Adaptor_V1 import meta_train_adaptor,specific_learning_UTAL

# changed the G generated by itself and add the mask to guide the Infor part

class Specific_Learning():
   def __init__(self,opt,device):
       self.opt = opt
       self.self_train = specific_learning_UTAL
       self.device = device
   def __call__(self,LR_HSI, HR_MSI):
       out = self.self_train(LR_HSI, HR_MSI,self.opt,self.device)

class ThreeBranch_Net(nn.Module):

   def __init__(self, opt,device ):
       super(ThreeBranch_Net, self).__init__()
       Dim =[opt.msi_channel,opt.msi_channel+opt.hsi_channel,opt.hsi_channel]
       self.sf = opt.sf
       Depth = opt.Depth
       KS_1 = opt.KS_1
       KS_2 = opt.KS_2
       KS_3 = opt.KS_3
       h_hrmsi , w_hrmsi =opt.h_size
       block1_1 = []
       block1_2 = []
       block2_1 = []
       block2_2 = []
       block3_1 = []
       block3_2 = []

       for i in range(Depth):
           block1_1 += [nn.Conv2d(128, 128, KS_1, 1, int(KS_1 / 2)), nn.ReLU()]
           block1_2 += [nn.Conv2d(128, 128, KS_1, 1, int(KS_1 / 2)), nn.ReLU()]
           block2_1 += [nn.Conv2d(128, 128, KS_2, 1, int(KS_2 / 2)), nn.ReLU()]
           block2_2 += [nn.Conv2d(128, 128, KS_2, 1, int(KS_2 / 2)), nn.ReLU()]
           block3_1 += [nn.Conv2d(128, 128, KS_3, 1, int(KS_3 / 2)), nn.ReLU()]
           block3_2 += [nn.Conv2d(128, 128, KS_3, 1, int(KS_3 / 2)), nn.ReLU()]

       self.layerIn1_1 = nn.Conv2d(Dim[0], 64, 3, 1, 1)
       self.layerIn1_2 = nn.Conv2d(64, 128, 3, 1, 1)
       self.layerIn2_1 = nn.Conv2d(Dim[1], 64, 3, 1, 1)
       self.layerIn2_2 = nn.Conv2d(64, 128, 3, 1, 1)
       self.layerIn3_1 = nn.Conv2d(Dim[2], 64, 3, 1, 1)
       self.layerIn3_2 = nn.Conv2d(64, 128, 3, 1, 1)

       # Shared Imformation extraction layer, between three input
       self.Infor = nn.Sequential(
           *[
               nn.Conv2d(387, 128, 3, 1, 1),
               nn.ReLU(),
               nn.Conv2d(128, 128, 3, 1, 1),
               nn.ReLU(),
               nn.Conv2d(128, 128, 3, 1, 1),
           ]
       )

       # Shared Imformation exchange layer between two stage
       self.layerX_G = nn.Conv2d(387, 128, 3, 1, 1)
       self.layerX_M = nn.Conv2d(128, 128, 3, 1, 1)
       self.layerX_B = nn.Conv2d(128, 128, 3, 1, 1)

       self.layerY_G = nn.Conv2d(387, 128, 3, 1, 1)
       self.layerY_M = nn.Conv2d(128, 128, 3, 1, 1)
       self.layerY_B = nn.Conv2d(128, 128, 3, 1, 1)

       self.layerZ_G = nn.Conv2d(387, 128, 3, 1, 1)
       self.layerZ_M = nn.Conv2d(128, 128, 3, 1, 1)
       self.layerZ_B = nn.Conv2d(128, 128, 3, 1, 1)

       self.layerOut1 = nn.Conv2d(384, 256, 3, 1, 1)
       self.layerOut2 = nn.Conv2d(256, 128, 3, 1, 1)
       self.layerOut3 = nn.Conv2d(128, Dim[-1], 3, 1, 1)

       # backbone of three branch in two stage
       self.branch1_1 = nn.Sequential(*block1_1)
       self.branch1_2 = nn.Sequential(*block1_2)
       self.branch2_1 = nn.Sequential(*block2_1)
       self.branch2_2 = nn.Sequential(*block2_2)
       self.branch3_1 = nn.Sequential(*block3_1)
       self.branch3_2 = nn.Sequential(*block3_2)

       self.Relu = nn.ReLU()
       self.Sig = nn.Sigmoid()



       #  shape: HR_MSI.size
       a = torch.ones(h_hrmsi,w_hrmsi ).unsqueeze(0)
       b = torch.zeros(h_hrmsi,w_hrmsi ).unsqueeze(0)
       self.MaskX = torch.cat((b, a, a), 0).unsqueeze(0).to(device)
       self.MaskY = torch.cat((a, b, a), 0).unsqueeze(0).to(device)
       self.MaskZ = torch.cat((a, a, b), 0).unsqueeze(0).to(device)

   def forward(self,lrhsi, hrmsi):
       '''
       x:HR_MSI
       y:torch.cat((UP_HSI,HR_MSI),1)
       z:UP_HSI
       '''
       x = hrmsi

       z = torch.nn.functional.upsample(lrhsi,scale_factor=self.sf,mode='bilinear')


       y = torch.cat((z,x),1)

       # Input processing
       outIn_1 = self.layerIn1_2(self.Relu(self.layerIn1_1(x)))
       outIn_2 = self.layerIn2_2(self.Relu(self.layerIn2_1(y)))
       outIn_3 = self.layerIn3_2(self.Relu(self.layerIn3_1(z)))

       # First stage
       out1 = self.branch1_1(outIn_1)
       out2 = self.branch2_1(outIn_2)
       out3 = self.branch3_1(outIn_3)

       Infor_x = self.Infor(torch.cat((out1, out2, out3, self.MaskX.repeat(out1.shape[0], 1, 1, 1)), 1))
       out1_G = self.Sig(self.layerX_G(torch.cat((out1, out2, out3, self.MaskX.repeat(out1.shape[0], 1, 1, 1)), 1)))
       out1_M = self.layerX_M(Infor_x)
       out1_B = self.layerX_B(Infor_x)
       out_1 = out1_G * out1 + (1 - out1_G) * (out1 * out1_M + out1_B)

       Infor_y = self.Infor(torch.cat((out1, out2, out3, self.MaskY.repeat(out1.shape[0], 1, 1, 1)), 1))
       out2_G = self.Sig(self.layerY_G(torch.cat((out1, out2, out3, self.MaskY.repeat(out1.shape[0], 1, 1, 1)), 1)))
       out2_M = self.layerY_M(Infor_y)
       out2_B = self.layerY_B(Infor_y)
       out_2 = out2_G * out2 + (1 - out2_G) * (out2 * out2_M + out2_B)

       Infor_Z = self.Infor(torch.cat((out1, out2, out3, self.MaskZ.repeat(out1.shape[0], 1, 1, 1)), 1))
       out3_G = self.Sig(self.layerZ_G(torch.cat((out1, out2, out3, self.MaskZ.repeat(out1.shape[0], 1, 1, 1)), 1)))
       out3_M = self.layerZ_M(Infor_Z)
       out3_B = self.layerZ_B(Infor_Z)
       out_3 = out3_G * out3 + (1 - out3_G) * (out3 * out3_M + out3_B)

       # Second Stage
       out1 = self.branch1_2(out_1)
       out2 = self.branch2_2(out_2)
       out3 = self.branch3_2(out_3)

       Infor_x = self.Infor(torch.cat((out1, out2, out3, self.MaskX.repeat(out1.shape[0], 1, 1, 1)), 1))
       out1_G = self.Sig(self.layerX_G(torch.cat((out1, out2, out3, self.MaskX.repeat(out1.shape[0], 1, 1, 1)), 1)))
       out1_M = self.layerX_M(Infor_x)
       out1_B = self.layerX_B(Infor_x)
       out_1 = out1_G * out1 + (1 - out1_G) * (out1 * out1_M + out1_B) + outIn_1

       Infor_y = self.Infor(torch.cat((out1, out2, out3, self.MaskY.repeat(out1.shape[0], 1, 1, 1)), 1))
       out2_G = self.Sig(self.layerY_G(torch.cat((out1, out2, out3, self.MaskY.repeat(out1.shape[0], 1, 1, 1)), 1)))
       out2_M = self.layerY_M(Infor_y)
       out2_B = self.layerY_B(Infor_y)
       out_2 = out2_G * out2 + (1 - out2_G) * (out2 * out2_M + out2_B) + outIn_2

       Infor_Z = self.Infor(torch.cat((out1, out2, out3, self.MaskZ.repeat(out1.shape[0], 1, 1, 1)), 1))
       out3_G = self.Sig(self.layerZ_G(torch.cat((out1, out2, out3, self.MaskZ.repeat(out1.shape[0], 1, 1, 1)), 1)))
       out3_M = self.layerZ_M(Infor_Z)
       out3_B = self.layerZ_B(Infor_Z)
       out_3 = out3_G * out3 + (1 - out3_G) * (out3 * out3_M + out3_B) + outIn_3

       # Output processing
       out = self.Relu(self.layerOut1(torch.cat((out_1, out_2, out_3), 1)))
       out = self.Relu(self.layerOut2(out))
       out = self.layerOut3(out)

       return out

class FineNet_SelfAtt_InputK_P_V2(nn.Module):

   def __init__(self, Dim=[31,3]):
       super(FineNet_SelfAtt_InputK_P_V2, self).__init__()
       self.Conv1 = nn.Conv2d(31, 31, 3, 1, 1)
       self.Conv2 = nn.Conv2d(31, 31, 3, 1, 1)
       self.Conv3 = nn.Conv2d(31, 31, 3, 1, 1)
       self.Conv4 = nn.Conv2d(32, 31, 3, 1, 1)
       self.Conv5 = nn.Conv2d(32, 31, 3, 1, 1)
       self.Conv6 = nn.Conv2d(62, 31, 3, 1, 1)
       self.Conv7 = nn.Conv2d(62, 31, 3, 1, 1)
       #self.Conv6 = nn.Conv2d(93, 31, 3, 1, 1)
       #self.Conv7 = nn.Conv2d(93, 31, 3, 1, 1)
       #self.Conv6 = nn.Conv2d(31, 31, 3, 1, 1)
       #self.Conv7 = nn.Conv2d(31, 31, 3, 1, 1)
       self.Relu = nn.ReLU(inplace=True)
       self.Linear_1 = nn.Linear(31, 16)
       self.Linear_2 = nn.Linear(16, 31)
       self.Sig  = nn.Sigmoid()

       # Upsample K, P
       Mid_Chas = 256
       self.layerIn_K_1 = nn.Conv2d(1, 64, 3, 1, 1)
       self.layerIn_K_2 = nn.Conv2d(64, Mid_Chas, 3, 1, 1)

       self.layerIn_P_1 = nn.Conv2d(1, 64, 3, 1, 1)
       self.layerIn_P_2 = nn.Conv2d(64, Mid_Chas, 3, 1, 1)
       self.layerIn_P_3 = nn.Linear(Dim[1], Dim[0])

       self.PixShuf = nn.PixelShuffle(16)

   def forward(self, x, K, P):
       # Upsample K, P into feature map
       [_, _, m, n] = x.shape
       F_K = self.layerIn_K_2(self.Relu(self.layerIn_K_1(K)))
       F_K = nn.functional.upsample(self.PixShuf(F_K), (m, n), mode='bilinear')
       F_P = self.layerIn_P_3(self.layerIn_P_2(self.Relu(self.layerIn_P_1(P))))
       F_P = nn.functional.upsample(self.PixShuf(F_P), (m, n), mode='bilinear')

       #out = self.Conv1(torch.cat((x, F_K, F_P), 1))
       out = self.Conv1(x)
       out = self.Conv2(self.Relu(out))
       out = self.Conv3(self.Relu(out))

       out_a = self.Conv5(torch.cat((out, F_K), 1)).mean(1).unsqueeze(1).repeat(1,x.shape[1],1,1)
       out_b = self.Conv4(torch.cat((out, F_P), 1))
       SE_b  = self.Sig(self.Linear_2(self.Relu(self.Linear_1(out_b.mean(3).mean(2))))).unsqueeze(2).unsqueeze(3)
       out_b = SE_b*out_b

       #Z = out_a*out + out_b*out
       Z = out_a*out + out_b

       Z = self.Conv6(self.Relu(torch.cat((out, Z), 1)))
       M = self.Sig(self.Conv7(self.Relu(torch.cat((out, Z), 1))))


       out = M*out + (1-M)*Z

       return out + x

class Meta_train():
   def __init__(self,opt,device):
       self.opt = opt
       self.meta_train = meta_train_adaptor
       self.device = device
       self.fusion = ThreeBranch_Net(opt,device)
   def __call__(self,x,idx):
       self.meta_train(x,idx,self.opt,self.device,self.fusion)

if __name__ =='__main__':
   import config
   opt = config.args_parser()
   model = ThreeBranch_Net(opt,'cpu')

   hrmsi = torch.rand([1,4,320,320])
   lrhsi = torch.rand([1,120,10,10])
   rec = model(hrmsi,lrhsi)
   print(rec.shape)

